---
permalink: /
title: ""
excerpt: ""
author_profile: true
redirect_from: 
  - /about/
  - /about.html
---

{% if site.google_scholar_stats_use_cdn %}
{% assign gsDataBaseUrl = "https://cdn.jsdelivr.net/gh/" | append: site.repository | append: "@" %}
{% else %}
{% assign gsDataBaseUrl = "https://raw.githubusercontent.com/" | append: site.repository | append: "/" %}
{% endif %}
{% assign url = gsDataBaseUrl | append: "google-scholar-stats/gs_data_shieldsio.json" %}

<span class='anchor' id='about-me'></span>

Hi! I am a data science master's student at Brown Universityüêª, where I am working with several amazing professors. I am working with Prof. [Stephen Bach](https://cs.brown.edu/people/sbach/) in the BATS Lab on adapting LLMs to out-of-distribution tasks parameter-efficiently. I am also working with Prof. [Shekhar Pradhan](https://vivo.brown.edu/display/spradh15) and Prof. [Ritambhara Singh](https://ritambharasingh.com/) in the Conversational AI Lab on improving VLMs understanding of relations and orders. Starting this July, I will join TikTok as a Machine Learning Engineer!

[\\]: <! I am expected to graduate in 2024 and am actively applying for Ph.D. programs related to computer science, data science, and artificial intelligence. >

[\\]: <! When I am not in the middle of a research project, I always find myself interested in implementing some fascinating deep-learning algorithms. I implemented a series of Transformer-based models, including Transformer, GPT, BERT, ViT, and CLIP, in my [TransformerHub](https://github.com/BubbleJoe-BrownU/TransformerHub), which is maintained as a long-term project serving the purpose of self-learning. I got a feeling of the dark magic of self-supervised learning (SSL) in my project [GourmAIt](https://github.com/BubbleJoe-BrownU/GourmAIt), where I implemented the Noisy Student Training and ResNets with stochastic depth. I implemented a CycleGAN and a gradient-based method in my project NeuralStyleTransfer. I also attempted to produce music using an RNN, a Convolutional RNN, and a Transformer. >

[\\]: <! I have comprehensive knowledge of machine learning and deep learning algorithms and have implemented many of them, including but not limited to CNNs, RNNs, DGNs, GANs. I also implemented many Transformer models, including Transformer, GPT, BERT, and ViT, and plan to implement more in my ongoing project. I have plenty of experience with interacting with CLIP and GPT for zero-shot learning, generating pseudo labels, and conducting RLAIF, namely reinforcing learning from AI feedback. >

Previously, in 2022, I received my bachelor's degree with honors at <a href='http://ckc.zju.edu.cn/ckcen/wbout/list.htm'>Cho Kochen Honors College</a>, Zhejiang University. I also got a mini-minor certificate in computer science during 2021 - 2022.

Email:  echo @gmail.com | sed 's/^/jiayuzheng99/'

To learn more about me, please check my <a href='https://bubblejoe-brownu.github.io/materials/jiayu_zheng_resume.pdf'>CV</a>.

# üîç Current Research Interests

My research experience largely lies in **Natural Language Processing (NLP)** and **Vision-Language Models (VLMs)**. Currently, I am doing research on improving language model performance by leveraging images as additional supervision signals at the Conversational AI Lab. My current research interests include three aspects:
- **Efficient NLP&VLM**: Improving the performance of LLM or VLM systems on out-of-domain (OOD) tasks, e.g. legal document classification for LLMs, and medical image classification for VLMs, in a parameter-efficient and data-efficient manner. In this direction, I have expertise in (1) **PEFT tuning of LLMs and VLMs**, developed transferable reasoning soft prompts to adapt Mistral-7b-instruct to downstream NLP tasks, LoRA-tuned Llama-2, Mistral, and Gemma on an OOD legal document classification task; (2) **knowledge distillation of LLMs**, trained smaller LLMs with synthetic data generated by larger ones, and implemented algorithms like Noisy Student Training; and (3) **tuning-free LLM alignment**, like controllable text generation and alignment of LLMs with In-Context Learning (ICL), e.g. Untuned LLMs with Restyled In-context ALignment (URIAL).

[//]: <! This goal could be approached on both the algorithm side and the hardware side. There has been research in model compression, like knowledge distillation, to enable smaller student models to predict like large teacher models. Works like Sparse Transformer, Longformer, and BigBird provide architectural innovations like shifted window attention and dilated attention, which reduce the formidable quadratic memory requirement without significant loss of performance. On the hardware side, carefully managing KV cache and offloading part of the model to CPU RAM lead to significantly improved throughput during inference. >
- **Explainable Reasoning in LLMs&VLMs**: I am interested in improving the reliability and factuality of LLMs chain-of-thought (CoT) reasoning ability with In-Context Learning. I would also want to understand LLM behaviors systematically and top-down, and explain them with studies of latent variables and neuron behaviors. I am also excited to improve VLMs' compositional reasoning ability, e.g. attribute object binding and relational understanding with specialized variants of contrastive learning objectives. I developed Relation-CLIP with hard negative mining and *relation-aware contrastive loss* that can distinguish relationships between or orders of subjects. In the long term, I believe in the potential of multimodal learning to enhance both language modeling and computer vision. 

[//]: <! , especially compositional reasoning, on synthetic or real-world datasets. Large language models, like GPT-4, display strong instruction following intelligence and, in the meanwhile, miserable reasoning ability. They perform poorly on math, easily shift their response if you contradict them, and are not able to reverse their reasoning. >


[//]: <! - (3) To **Next-generation model architectures**:  Using soft prompts, i.e. caching a small, dataset-specific, learnable vocabulary for downstream tasks, can adapt large models towards specific domains, or remedy problems brought by subword-lebel tokenizers. This has proven to be effective in increasing LLM's performance in mathematical calculations, legal provision classification, or medical image classification tasks. >


# üî• News
- *2024.06*: &nbsp;üéâüéâ I'll join TikTok as a Machine Learning Engineer.
- *2023.06*: &nbsp;üéâüéâ Research internship at Conversational AI Lab, Brown University.
- *2022.09*: &nbsp;üéâüéâ I'll be joining Brown University as a data science MS student.
- *2022.06*: &nbsp;üéâüéâ Graduated from Zhejiang University as Outstanding Graduates (rank 2% among a cohort of 101 students).

# üìù Publications and Projects

<div class='paper-box'><div class='paper-box-image'><div><div class="badge">Project 2023</div><img src='images/noisy_student.png' alt="sym" width="50%"></div></div>
<div class='paper-box-text' markdown="1">

[GourmAIt: Training Robust Deep Neural Networks on Noisy Datasets](https://github.com/BubbleJoe-BrownU/GourmAIt)

**Jiayu Zheng**

[[project page](https://github.com/BubbleJoe-BrownU/GourmAIt)]

- Final project for CS141 Computer Vision
- Implemented Noisy Student Training, a self-supervised learning (SSL) algorithm published by Google Research, where pseudo labels generated by a teacher model are used to train a
larger-or-equal-size student model, which becomes the teacher model in the next iteration
- Implemented ResNet with stochastic depth, step-wise unfreezing scheduling, and learning rate scheduling to maximize the performance gain in the fine-tuning phase
</div>
</div>

<div class='paper-box'><div class='paper-box-image'><div><div class="badge">Project 2023</div><img src='images/transformer_hub_logo.jpg' alt="sym" width="50%"></div></div>
<div class='paper-box-text' markdown="1">

[TransformerHub: A Collection of Transformer-Based Models](https://github.com/BubbleJoe-BrownU/TransformerHub)

**Jiayu Zheng**

[[project page](https://github.com/BubbleJoe-BrownU/TransformerHub)]

- Long-term project for self-training, adopted from my final project for CS247 Deep Learning
- Implemented various encoder-only, decoder-only, and encoder-decoder Transformer models including Transformer, BERT, GPT, ViT, and CLIP
- Implemented advanced features like prefix causal attention, sliding window attention, rotary position embedding, extrapolable position embedding
</div>
</div>

<div class='paper-box'><div class='paper-box-image'><div><div class="badge">Project 2022</div><img src='images/cyber_barista_logo.jpg' alt="sym" width="50%"></div></div>
<div class='paper-box-text' markdown="1">

[Cyber Barista: Coffee Bean Quality Prediction With Ensemble Learning](https://github.com/BubbleJoe-BrownU/hands-on-machine-learning-project)

**Jiayu Zheng**

[[project page](https://github.com/BubbleJoe-BrownU/hands-on-machine-learning-project)]

- Course project for DATA1030 Data Science
- Implemented an ensemble of nine models, including an ElasticNet, an XGBoost, a multi-layer perceptron (MLP), etc., to predict the quality score of coffee beans
- Interpreted the model output using feature-based methods: random perturbation, weight magnitude, and SHAP
</div>
</div>

<div class='paper-box'><div class='paper-box-image'><div><div class="badge">Synlett 2020</div><img src='images/chem_research_pic.png' alt="sym" width="50%"></div></div>
<div class='paper-box-text' markdown="1">

[A Molecular Stereostructure Descriptor Based On Spherical Projection](https://www.thieme-connect.com/products/ejournals/pdf/10.1055/s-0040-1705977.pdf)

Licheng Xu, Xin Li, Miaojiong Tang, Luotian Yuan, **Jiayu Zheng**, Shuoqing Zhang, Xin Hong

[[project page](https://github.com/licheng-xu-echo/SPMS)]    [[paper](https://www.thieme-connect.com/products/ejournals/pdf/10.1055/s-0040-1705977.pdf)]

- A numeric descriptor that converts the spatial, continuous van der Waals potential field of molecules to a sequence of 2D matrics
- Trained a Convolutional Neural Network (CNN) to predict the stereo-selectivity of chemical reactions
</div>
</div>

# üéñ Honors and Awards
- *2022.06* Outstanding Graduates of Zhejiang University
- *2022.02* Zhejiang University Scholarship, First Prize (top 3%)
- *2021.12* Cho Kochen Honors College Second-Class Scholarship for Elite Students in Basic Sciences
- *2021.12* Cho Kochen Honors College Scholarship for Innovation (international contest award)
- *2021.01* Zhejiang University Scholarship, Third Prize (top 15%)
- *2020.01* Zhejiang University Scholarship, First Prize (top 3%)
- *2020.01* Outstanding Students. 

# üìñ Educations
- *2022.09 - Present*, *Sc.M.* in Data Science, Data Science Institute, Brown University. 
- *2018.09 - 2022.06*, *Sc.B.* in Chemistry with Honors, Cho KoChen Honors College, Zhejiang University. 

# üí¨ Services
- *2024.01 - 2024.05*, Teaching Assistant (upcoming), CS1430 Computer Vision, Brown University
- *2023.09 - Present*, Teaching Assistant, CS1410 Artificial Intelligence, Brown University.
- *2023.04 - 2023.04*, Mentor for College-day Data Science Bootcamp, Brown University.
- *2023.03 - 2023.03*, Mentor for Women in Data Science (WiDS), Brown University.
- *2020.02 - 2020.10*, Head Teaching Assistant, Structural Chemistry and Spectroscopy, Zhejiang University.


# üíª Internships
- *2023.06 - 2023.09*, Research Internship, Conversational AI Lab, Brown University.

# ü™Ç Misc
I am a loyal fan of Ubisoft's [Assassin's Creed](https://en.wikipedia.org/wiki/Assassin%27s_Creed) franchise. Take a leap of faith to the unknown!

As a huge consumer of espresso-based coffees, I am dreaming of starting my coffee shop one day.
My favorites are:
- Latte
- Flat White
- Cortado
- Macchiato

[//]: <! I have not done and will not do the MBTI test because I doubt its scientific grounding. Plus, I believe a human being in flesh should not be simply defined by a few tags. People should be encouraged to break boundaries and shape themselves with the current and the future in a Markovian way. >
