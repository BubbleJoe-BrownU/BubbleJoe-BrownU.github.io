---
permalink: /
title: ""
excerpt: ""
author_profile: true
redirect_from: 
  - /about/
  - /about.html
---

{% if site.google_scholar_stats_use_cdn %}
{% assign gsDataBaseUrl = "https://cdn.jsdelivr.net/gh/" | append: site.repository | append: "@" %}
{% else %}
{% assign gsDataBaseUrl = "https://raw.githubusercontent.com/" | append: site.repository | append: "/" %}
{% endif %}
{% assign url = gsDataBaseUrl | append: "google-scholar-stats/gs_data_shieldsio.json" %}

<span class='anchor' id='about-me'></span>

Hi! I am a data science master's student at Brown University, where I work with Shekhar Pradhan and Ritambhara Singh in Conversational AI Lab as a research assistant. I am expected to graduate in 2024 and am actively applying for Ph.D. programs related to computer science, data science, and artificial intelligence.

I have comprehensive knowledge of machine learning and deep learning algorithms and have implemented many of them, including but not limited to CNNs, RNNs, DGNs, GANs. I also implemented many Transformer models, including Transformer, GPT, BERT, and ViT, and plan to implement more in my ongoing project [TransformerHub](https://github.com/BubbleJoe-BrownU/TransformerHub). I have plenty of experience with interacting with CLIP and GPT for zero-shot learning, generating pseudo labels, and conducting RLAIF, namely reinforcing learning from AI feedback. I am also familiar with techniques that allow today's large models to train and inference, like gradient accumulation, mixed precision training, distributed data parallelism, ZeRO parallelism, etc.

Previously, in 2022, I received my bachelor's degree with honors at <a href='http://ckc.zju.edu.cn/ckcen/wbout/list.htm'>Cho Kochen Honors College</a>, Zhejiang University. I also got a mini-minor certificate in computer science during 2021 - 2022.

Email: jiayu_zheng (at) brown.edu

My <a href='https://drive.google.com/file/d/1s-rXey7FF44fQ7-nKuw1CPvRphzxEUey/view'>CV</a> (up to August 2023) can be viewed here.

# üîç Current Research Interests

Currently, I am researching improving language model performance by leveraging images as additional supervision signals at the Conversational AI Lab. My research interests include three aspects. My research experience largely lies in computational linguistics and vision-language models. My current research interests include
- (1) leveraging large language models and vision-language models to generate pseudo labels for unlabeled datasets or noisy datasets, which provides high-quantity and high-quality labels for fast deployment of machine learning systems.
- (2) To make high-performance models more memory- and compute-efficient. There has been research in model compression, like knowledge distillation, to enable smaller student models to predict like large teacher models. Works like Sparse Transformer, Longformer, and BigBird provide architectural innovations like shifted window attention and dilated attention, which reduce the formidable quadratic memory requirement of Transformer models from $O(N^2)$ to $O(log(N))$, even $O(N)$, without significant loss of performance.
- (3) To investigate large language models' and vision-language models' reasoning ability, especially compositional reasoning, on synthetic or real-world datasets. Large language models, like GPT-4, display strong instruction following intelligence and, in the meanwhile, miserable reasoning ability. They perform poorly on math, easily shift their response if you contradict them, and are not able to reverse their reasoning.
- (4) Improving large language models' and vision-language model's performance on downstream tasks parameter-efficiently. Using soft prompts, i.e. caching a small, dataset-specific, learnable vocabulary for downstream tasks, can adapt large models towards specific domains, or remedy problems brought by subword-lebel tokenizers. This has proven to be effective in increasing LLM's performance in mathematical calculations, legal provision classification, or medical image classification tasks.


# üî• News
- *2023.06*: &nbsp;üéâüéâ Research intern at Conversational AI Lab, Brown University.
- *2022.09*: &nbsp;üéâüéâ I'll be joining Brown University as a data science MS student.
- *2022.06*: &nbsp;üéâüéâ Graduated from Zhejiang University as Outstanding Graduates.

# üìù Publications and Projects

<div class='paper-box'><div class='paper-box-image'><div><div class="badge">Project 2023</div><img src='images/noisy_student.png' alt="sym" width="50%"></div></div>
<div class='paper-box-text' markdown="1">

[GourmAIt: Training Robust Deep Neural Networks on Noisy Datasets](https://github.com/BubbleJoe-BrownU/GourmAIt)

Jiayu Zheng
- Implemented Noisy Student Training, where pseudo labels generated by a teacher model are used to train a
larger-or-equal-size student model, which becomes the teacher model in the next iteration
- Implemented stochastic depth, stepwise unfreezing scheduling, and learning rate scheduling to maximize the performance gain in finetuning ResNet
</div>
</div>

- *2020* A Molecular Stereostructure Descriptor Based On Spherical Projection, Licheng Xu, Xin Li, Miaojiong Tang, Luotian Yuan, **Jiayu Zheng**, Shuoqing Zhang, Xin Hong

# üéñ Honors and Awards
- *2022.05* Outstanding Graduates.
- *2021.10* Zhejiang University Scholarship, First Prize.
- *2019.10* Zhejiang University Scholarship, First Prize. 
- *2019.09* Outstanding Students. 

# üìñ Educations
- *2022.09 - Present*, *Sc.M* in Data Science, Data Science Institute, Brown University. 
- *2018.09 - 2022.06*, *Sc.B* in Chemistry with Honors, Cho KoChen Honors College, Zhejiang University. 

# üí¨ Services
- *2023.09 - Present*, Teaching Assistant, CS1410 Artificial Intelligence, Brown University.
- *2022.03 - 2022.03*, Mentor for Women in Data Science (WiDS), Brown University.
- *2020.02 - 2020.10*, Undergraduate Head TA, Structural Chemistry, Zhejiang University.


# üíª Internships
- *2023.06 - 2023.09*, Research Internship, Conversational AI Lab, Brown University.
